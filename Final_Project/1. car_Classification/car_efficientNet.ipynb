{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import Subset\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms,datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "model_name = \"efficientnet-b0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data 불러오기\n",
    "- transforms.Compose 안의 Resize, ToTensor, Normalize 기능으로 다양한 크기의 이미지들을 한번에 정돈하여 불러옵니다.\n",
    "- mageNet은 입력이 224x224 형식이므로 이에 맞춰 Resize 해줍니다.  torch의 입력형태인 Tensor로 바꿔 준 후 Normalize 해줍니다.\n",
    "- sklearn의 train_test_split 함수로 전체 데이터를 train/valid/test 셋으로 나눠줍니다. (8:1:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "print(os.getcwd())  # 현재 작업 디렉토리를 출력합니다.\n",
    "\n",
    "data_path= \"/content/drive/MyDrive/9. newdata/nnnnewdata\"\n",
    "# ImageFolder 함수를 사용해 이미지 데이터셋을 불러오며, Compose를 사용해 이미지에 변환작업 진행.\n",
    "car_dataset = datasets.ImageFolder(data_path,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
    "    ]))\n",
    "train_idx, tmp_idx = train_test_split(list(range(len(car_dataset))), test_size=0.2, random_state=random_seed)\n",
    "datasets = {}\n",
    "datasets['train'] = Subset(car_dataset, train_idx)\n",
    "tmp_dataset       = Subset(car_dataset, tmp_idx)\n",
    "\n",
    "val_idx, test_idx = train_test_split(list(range(len(tmp_dataset))), test_size=0.5, random_state=random_seed)\n",
    "datasets['valid'] = Subset(tmp_dataset, val_idx)\n",
    "datasets['test']  = Subset(tmp_dataset, test_idx)\n",
    "\n",
    "## data loader\n",
    "dataloaders, batch_num = {}, {}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(datasets['train'],\n",
    "                                                batch_size=batch_size, shuffle=True,\n",
    "                                                num_workers=4)\n",
    "dataloaders['valid'] = torch.utils.data.DataLoader(datasets['valid'],\n",
    "                                                batch_size=batch_size, shuffle=False,\n",
    "                                                num_workers=4)\n",
    "dataloaders['test']  = torch.utils.data.DataLoader(datasets['test'],\n",
    "                                                batch_size=batch_size, shuffle=False,\n",
    "                                                num_workers=4)\n",
    "batch_num['train'], batch_num['valid'], batch_num['test'] = len(dataloaders['train']), len(dataloaders['valid']), len(dataloaders['test'])\n",
    "print('batch_size : %d,  tvt : %d / %d / %d' % (batch_size, batch_num['train'], batch_num['valid'], batch_num['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data 확인\n",
    "- dataset.ImagFolder를 잘 가져 오는지 확인합니다.\n",
    "- transfoms로 tensor로 변환된 이미지를 불러와 class이름과 같이 확인 하는 작업을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))  # 텐서를 넘파이 배열로 변환하고, 축을 변경합니다.\n",
    "    mean = np.array([0.485, 0.456, 0.406])  # 정규화를 해제하기 위한 평균 값입니다.\n",
    "    std = np.array([0.229, 0.224, 0.225])   # 정규화를 해제하기 위한 표준편차 값입니다.\n",
    "    inp = std * inp + mean  # 정규화를 해제합니다.\n",
    "    inp = np.clip(inp, 0, 1)  # 이미지 배열의 값이 0과 1 사이의 값이 되도록 합니다.\n",
    "    plt.imshow(inp)  # 이미지를 출력합니다.\n",
    "    if title is not None:  # 만약 타이틀이 있다면,\n",
    "        plt.title(title)  # 그 타이틀을 출력합니다.\n",
    "    plt.pause(0.001)  # 그림이 업데이트 되는 것을 잠시 멈춥니다.\n",
    "\n",
    "num_show_img = 5  # 보여줄 이미지의 수를 설정합니다.\n",
    "class_names={   # 클래스 이름을 사전형으로 설정합니다.\n",
    "    \"00\":\"MAXCRUZ\",\n",
    "    \"01\":\"MORNING\",\n",
    "    \"02\":\"SONATA\",\n",
    "    \"03\":\"TIVOLI\",\n",
    "    \"04\":\"SM5\",\n",
    "    \"05\":\"STINGER\",\n",
    "    \"06\":\"MOHAVE\",\n",
    "    \"07\":\"CARNIVAL\",\n",
    "    \"08\":\"TUCSON\",\n",
    "    \"09\":\"CLIO\"\n",
    "}\n",
    "# train, valid, test 이미지를 가져와 그디르 형태로 만듭니다.\n",
    "# train\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "out = torchvision.utils.make_grid(inputs[:num_show_img])\n",
    "imshow(out, title=[class_names[str(int(x))] for x in classes[:num_show_img]])\n",
    "# valid\n",
    "inputs, classes = next(iter(dataloaders['valid']))\n",
    "out = torchvision.utils.make_grid(inputs[:num_show_img])\n",
    "imshow(out, title=[class_names[str(int(x))] for x in classes[:num_show_img]])\n",
    "# test\n",
    "inputs, classes = next(iter(dataloaders['test']))\n",
    "out = torchvision.utils.make_grid(inputs[:num_show_img])\n",
    "imshow(out, title=[class_names[str(int(x))] for x in classes[:num_show_img]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 코드 작성\n",
    "- epoch을 돌때 마다 훈련하고, 평가를 진행하면서 평가 정확도가 높으면 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    # 학습 시작 시간을 기록\n",
    "    since = time.time()\n",
    "    # 최적의 모델 가중치를 저장할 변수를 초기화합니다.\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    train_loss, train_acc, valid_loss, valid_acc = [], [], [], []\n",
    "    # 지정된 에포크만큼 반복하여 학습을 수행합니다.\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        # 각 에포크에서는 학습 단계와 검증 단계를 모두 실행합니다.\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 학습 모드로 모델을 설정합니다.\n",
    "            else:\n",
    "                model.eval()   # 검증 모드로 모델을 설정합니다.\n",
    "            running_loss, running_corrects, num_cnt = 0.0, 0, 0\n",
    "            # 데이터 로더에서 배치 단위로 데이터를 가져와 처리합니다.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # 모델의 파라미터 그라디언트를 초기화합니다.\n",
    "                optimizer.zero_grad()\n",
    "                # 학습 단계에서만 그라디언트를 계산하도록 설정합니다.\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)  # 모델의 예측값을 계산합니다.\n",
    "                    _, preds = torch.max(outputs, 1)  # 예측값 중 가장 높은 확률을 가진 클래스를 선택합니다.\n",
    "                    loss = criterion(outputs, labels)  # 손실을 계산합니다.\n",
    "                    # 학습 단계에서는 역전파를 수행하고, 옵티마이저를 이용해 파라미터를 업데이트합니다.\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # 역전파를 수행합니다.\n",
    "                        optimizer.step()  # 옵티마이저를 이용해 파라미터를 업데이트합니다.\n",
    "                # 통계를 계산합니다.\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                num_cnt += len(labels)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            epoch_loss = float(running_loss / num_cnt)\n",
    "            epoch_acc  = float((running_corrects.double() / num_cnt).cpu()*100)\n",
    "            # 학습과 검증 단계에서의 손실과 정확도를 기록합니다.\n",
    "            if phase == 'train':\n",
    "                train_loss.append(epoch_loss)\n",
    "                train_acc.append(epoch_acc)\n",
    "            else:\n",
    "                valid_loss.append(epoch_loss)\n",
    "                valid_acc.append(epoch_acc)\n",
    "            print('{} Loss: {:.2f} Acc: {:.1f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            # 검증 단계에서 정확도가 이전보다 좋을 경우, 모델의 상태를 저장합니다.\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_idx = epoch\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                print('==> best model saved - %d / %.1f'%(best_idx, best_acc))\n",
    "\n",
    "    # 학습에 걸린 시간을 계산하고 출력합니다.\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best valid Acc: %d - %.1f' %(best_idx, best_acc))\n",
    "    # 가장 성능이 좋았던 모델의 상태를 불러옵니다.\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), 'car_model.pt')  # 모델을 저장합니다.\n",
    "    print('model saved')\n",
    "    return model, best_idx, best_acc, train_loss, train_acc, valid_loss, valid_acc  # 학습 결과를 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 설정\n",
    "- GPU를 사용한다고 선언합니다.\n",
    "- 분류 문제이므로 CrossEntropy를 설정합니다. (Negative log를 씌운걸 쓰기도 하는데, SGD, RMS를 사용한 결과 SGD가 잘 작동하는것을 확인)\n",
    "- Lr를 미세하게 조정하여 최적의 값을 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA를 사용할 수 있는 경우 GPU를, 그렇지 않은 경우 CPU를 사용하도록 설정합니다.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  \n",
    "# 모델을 해당 장치로 이동시킵니다.\n",
    "model = model.to(device)\n",
    "# 손실 함수로 CrossEntropyLoss를 사용합니다.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# 옵티마이저로 Stochastic Gradient Descent (SGD)를 사용하며, 학습률은 0.01, 모멘텀은 0.9, 가중치 감쇠는 1e-4로 설정합니다.\n",
    "optimizer_ft = optim.SGD(model.parameters(),\n",
    "                        lr = 0.01,\n",
    "                        momentum=0.9,\n",
    "                        weight_decay=1e-4)\n",
    "# 학습률 스케줄러를 설정합니다. 각 에포크마다 학습률을 0.98739로 곱하여 학습률을 감소시킵니다.\n",
    "lmbda = lambda epoch: 0.98739\n",
    "exp_lr_scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer_ft, lr_lambda=lmbda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model 함수를 이용해 모델을 학습시킵니다. 이 함수는 학습된 모델, 가장 좋은 성능을 낸 에포크의 인덱스, \n",
    "# 그 때의 정확도, 각 에포크마다의 학습 손실과 정확도, 검증 손실과 정확도를 반환합니다.\n",
    "model, best_idx, best_acc, train_loss, train_acc, valid_loss, valid_acc = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
